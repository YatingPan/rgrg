{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from custom_image_dataset import CustomImageDataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"We use: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset csv files as pd dataframes and print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_chest_imagenome_customized = \"/u/home/tanida/datasets/chest-imagenome-dataset-customized-full\"\n",
    "\n",
    "# reduce memory usage by only using necessary columns and selecting appropriate datatypes\n",
    "usecols = [\"mimic_image_file_path\", \"bbox_name\", \"x1\", \"y1\", \"x2\", \"y2\", \"is_abnormal\"]\n",
    "dtype = {\"x1\": \"int16\", \"x2\": \"int16\", \"y1\": \"int16\", \"y2\": \"int16\", \"bbox_name\": \"category\"} \n",
    "\n",
    "datasets_as_dfs = {dataset: os.path.join(path_chest_imagenome_customized, dataset) + \".csv\" for dataset in [\"train\", \"valid\", \"test\"]}\n",
    "datasets_as_dfs = {dataset: pd.read_csv(csv_file_path, usecols=usecols, dtype=dtype) for dataset, csv_file_path in datasets_as_dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_num_samples_per_class(dataset, df):\n",
    "    print(f\"{dataset}:\")\n",
    "    for bbox_name, count in df[\"bbox_name\"].value_counts().iteritems():\n",
    "        print(f\"\\t{bbox_name}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of samples per class for each dataset\n",
    "\n",
    "# bboxes of anatomical regions are almost distributed equally in all datasets,\n",
    "# only a slight imbalance because not every image has bboxes of all 36 anatomical regions\n",
    "for dataset, df in datasets_as_dfs.items():\n",
    "    print_num_samples_per_class(dataset, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of samples for each dataset\n",
    "total_num_samples = sum(len(df) for df in datasets_as_dfs.values())\n",
    "\n",
    "for dataset, df in datasets_as_dfs.items():\n",
    "    print(f\"{dataset}: {len(df):,} samples ({(len(df) / total_num_samples) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we don't want to train on the full train set (with 5,950,238 samples), we can specify the constant below to limit the train set\n",
    "\n",
    "PERCENTAGE_OF_TRAIN_SET_TO_USE = 0.3\n",
    "total_num_samples_train = len(datasets_as_dfs[\"train\"])\n",
    "\n",
    "new_num_samples_train = int(PERCENTAGE_OF_TRAIN_SET_TO_USE * total_num_samples_train)\n",
    "\n",
    "datasets_as_dfs[\"train\"] = datasets_as_dfs[\"train\"][:new_num_samples_train]\n",
    "\n",
    "print(f\"train (new): {len(datasets_as_dfs['train']):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets as Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for image transformations\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "IMAGE_INPUT_SIZE = 224  # pre-trained DenseNet121 model expects images to be of size 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: transforms are applied to the already cropped images (see __getitem__ method of CustomImageDataset class)!\n",
    "\n",
    "# use albumentations for Compose and transforms\n",
    "train_transforms = A.Compose([\n",
    "    # we want the long edge of the image to be resized to IMAGE_INPUT_SIZE, and the short edge of the image to be padded to IMAGE_INPUT_SIZE on both sides,\n",
    "    # such that the aspect ratio of the images are kept (i.e. a resized image of a lung is not distorted), \n",
    "    # while getting images of uniform size (IMAGE_INPUT_SIZE x IMAGE_INPUT_SIZE)\n",
    "\n",
    "    A.LongestMaxSize(max_size=IMAGE_INPUT_SIZE, interpolation=cv2.INTER_AREA),  # resizes the longer edge to IMAGE_INPUT_SIZE while maintaining the aspect ratio (INTER_AREA works best for shrinking images)\n",
    "    A.PadIfNeeded(min_height=IMAGE_INPUT_SIZE, min_width=IMAGE_INPUT_SIZE, border_mode=cv2.BORDER_CONSTANT),  # pads both sides of the shorter edge with 0's (black pixels)\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# don't apply data augmentations to val and test set\n",
    "val_test_transforms = A.Compose([\n",
    "    A.LongestMaxSize(max_size=IMAGE_INPUT_SIZE, interpolation=cv2.INTER_AREA),\n",
    "    A.PadIfNeeded(min_height=IMAGE_INPUT_SIZE, min_width=IMAGE_INPUT_SIZE, border_mode=cv2.BORDER_CONSTANT),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(dataset_df=datasets_as_dfs[\"train\"], transforms=train_transforms)\n",
    "val_dataset = CustomImageDataset(dataset_df=datasets_as_dfs[\"vald\"], transforms=val_test_transforms)\n",
    "test_dataset = CustomImageDataset(dataset_df=datasets_as_dfs[\"test\"], transforms=val_test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    # we want the long edge of the image to be resized to IMAGE_INPUT_SIZE, and the short edge of the image to be padded to IMAGE_INPUT_SIZE on both sides,\n",
    "    # such that the aspect ratio of the images are kept (i.e. a resized image of a lung is not distorted), \n",
    "    # while getting images of uniform size (IMAGE_INPUT_SIZE x IMAGE_INPUT_SIZE)\n",
    "\n",
    "    # the custom class CustomResize resizes the longer edge to IMAGE_INPUT_SIZE while maintaining the aspect ratio\n",
    "    A.LongestMaxSize(max_size=224, interpolation=cv2.INTER_AREA),\n",
    "    A.PadIfNeeded(min_height=224, min_width=224, border_mode=cv2.BORDER_CONSTANT)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/u/home/tanida/datasets/mimic-cxr-jpg/files/p10/p10001401/s50225296/0009a9fb-eb905e90-824cad7c-16d40468-007f0038.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "np_cropped_image = image[409:1132, 1364:2182]  # img[Y:Y+H, X:X+W]\n",
    "print(np_cropped_image.shape)\n",
    "plt.imshow(np_cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/u/home/tanida/datasets/mimic-cxr-jpg/files/p10/p10001401/s50225296/0009a9fb-eb905e90-824cad7c-16d40468-007f0038.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "bbox_coords = [245, 1800, 2264, 3042]\n",
    "cropped_image = image.crop(box=bbox_coords)\n",
    "\n",
    "print(type(cropped_image))\n",
    "print(cropped_image.size)\n",
    "display(cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_resized_cropped_image = train_transforms(image=np_cropped_image)[\"image\"]\n",
    "print(new_resized_cropped_image.shape)\n",
    "plt.imshow(new_resized_cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_resized_cropped_image == resized_cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_cropped_image = transforms.functional.resize(cropped_image, size=223, max_size=224)\n",
    "print(resized_cropped_image)\n",
    "display(resized_cropped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d = datasets_as_dfs[\"train\"].head().iloc[2, 2:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path to folder where model weights should be saved\n",
    "model_save_path = \"/u/home/tanida/weights/encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af344a1d4a4009a9796a149eae461abe839e0a1e355ecc657514cba65d6053da"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cxr_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
